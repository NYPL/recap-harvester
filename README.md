# ReCAP Harvester

This application is a poller in the Data/Search pipeline of NYPL's Library Services Platform. It polls an S3 bucket (formerly an SFTP server) managed by SCSB for additions, updates, and deletions of partner records. It processes SCSBXML and json files and broadcasts Bib and Item documents into the `BibPostRequest-[env]` and `ItemPostRequest-[env]` streams just as our [Sierra Retriever](https://github.com/NYPL-discovery/sierra-retriever/) does for our own records. This app is represented by the "RecapHarvester" component in our [Data/Search Architecture diagram](https://docs.google.com/presentation/d/1kPUhT-JPOuniXndKWc_JEp2EY5rOPuH5ebSqYCe_438/edit#slide=id.g401dec0f26_0_128).

## Setup

This is a Java app built by [Apache Maven](https://maven.apache.org/what-is-maven.html).

To install Maven on OSX using [Homebrew](https://brew.sh/):

```
brew install mvn
```

To compile and run all tests after modifying code:

```
mvn clean package
```

The environmental config you'll need to set up depends on the mode in which you run the app, covered below.

Note on configuring logging: It's not clear how to modify the log level of this app via an environmental variable. It's stuck at level `INFO`.

## When does this app run?

This app can be run in 2 different modes.

1. *Nightly*: On a [nightly basis](#nightly), it downloads HTC-generated files that represent bibliographic additions, updates,
and deletions.
1. *Bulk*: The [initial seeding of our databases & index](#initial) from a massive zip of our partners' items generated by HTC. (Note: This was done at product launch and not retested since. We're not likely to need to do this again unless/until we have a very large influx of partner records to update.)

### "Nightly Updates" Mode

The app connects to an S3 bucket (formerly an SFTP server) managed by SCSB to retrieve partner bib and item metadata updates. It uses [Apache Camel](https://camel.apache.org/), a streaming framework, to manage the following rough pipeline:
 * Additions & Updates:
   * Fetch zipped SCSBXml documents representing additions and updates for partner records from S3 bucket in `data-exports/NYPL/SCSBXml/Incremental/*.zip`
   * (Move the processed zips to `data-exports/NYPL/SCSBXml/Incremental.processed` in the same bucket so that we don't process them again.)
   * Unzip the SCSBXML documents and write them locally
   * Iterate over the local SCSBXML documents, translate them into Bib and Item documents
   * Broadcast the Bib and Item documents over `BibPostRequest-[env]` and `ItemPostRequest-[env]` Kinesis streams, respectively (Avro encoded using namesake schemas)
 * Deletions:
   * Fetch zipped JSON documents representing deleted ("deaccessioned") partner bibs and items from `data-exports/NYPL/Json/*.zip`
   * (Move the processed zips to `data-exports/NYPL/Json.processed` in the same bucket so that we don't process them again.)
   * Unzip the enclosed JSON documents and write them locally
   * Iterate over the json files, translate them into Bib and Item documents with `"deleted": true`
   * Broadcast the "deleted" documents over `BibPostRequest-[env]` and `ItemPostRequest-[env]` Kinesis streams (Avro encoded using namesake schemas)

#### Running the app in "Nightly Updates" mode locally

See [.env-local-export.sample](.env-local-export.sample) for a sample config file you can use for running the app locally. To use it, copy `.env-local-export` to `.env-local-export` and fill in the missing values. This allows you to `source` the file into your shell and run the app:

```
source .env-local-export; mvn spring-boot:run
```

Note that above also auto-compiles any changes you've made, so there's no need to run `mvn clean package` before running the above.

### "Bulk" Mode

This is for the initial load of data. This mode is untested since launch and is unlikely to be necessary in the short term.

TODO: note saying that this requires manual downloading & unzipping from SFTP.

#### Running the app in "Initial Load" mode locally

The following environment variables need to be set. For different options to set AWS credentials, please refer to http://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html.

```
AWS_ACCESS_KEY_ID=[used-to-publish-to-kinesis]
AWS_SECRET_ACCESS_KEY=[used-to-publish-to-kinesis]
bibSchemaAPI=https://[domain.example.com]/api/v0.1/current-schemas/BibPostRequest
itemSchemaAPI=https://[domain.example.com]/api/v0.1/current-schemas/ItemPostRequest
kinesisBibStream=[snip]
kinesisItemStream=[snip]
scsbexportstagingLocation=/var/app/current/scsbxml
```

...can we add specific IDE instructions...(or things useful to future maintainers)

## Deployment

This app is deployed automatically by Travis when updates are made to the `qa` or `production` branch.

1.  `mvn clean package`
1.  `eb init Recap-Harvester --profile [profile name]`
1.  Create application

  ```bash
  eb create [nightly|initial]-recap-harvester-[environment] \
      --instance_type m3.medium \
      --instance_profile [nightly|initial]-cloudwatchable-beanstalk \
      --cname recap-harvester-[environment] \
      --single \
      --vpc.id env-vpc \
      --vpc.ec2subnets private-subnet-id-1 \
      --tags Project=Discovery,harvester=recap_harvester \
      --keyname dgdvteam \
      --envvars KEYFROMABOVE="value",KEYFROMABOVE2="value" \
      --profile your-aws-profile-name
  ```

## Contributing

This repo uses the [Main-QA-Production Git Workflow](https://github.com/NYPL/engineering-general/blob/master/standards/git-workflow.md#main-qa-production)
